"use strict";(globalThis.webpackChunkhackathon_book=globalThis.webpackChunkhackathon_book||[]).push([[828],{7877:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Part-4-vla-humanoids/VLA","title":"VLA","description":"Vision-Language-Action (VLA)","source":"@site/docs/Part-4-vla-humanoids/01-VLA.md","sourceDirName":"Part-4-vla-humanoids","slug":"/Part-4-vla-humanoids/VLA","permalink":"/hackathon-book/docs/Part-4-vla-humanoids/VLA","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Part-4-vla-humanoids/01-VLA.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"NVIDIA Isaac AI","permalink":"/hackathon-book/docs/Part-3-isaac/isaac-brain"},"next":{"title":"humainoids","permalink":"/hackathon-book/docs/Part-4-vla-humanoids/humainoids"}}');var o=i(4848),s=i(8453);const a={},r=void 0,l={},c=[{value:"Vision-Language-Action (VLA)",id:"vision-language-action-vla",level:2},{value:"<strong>5.1 The Cognitive Pipeline: Voice-to-Action</strong>",id:"51-the-cognitive-pipeline-voice-to-action",level:3},{value:"<strong>5.2 Visual Understanding for VLA</strong>",id:"52-visual-understanding-for-vla",level:3}];function d(n){const e={code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.h2,{id:"vision-language-action-vla",children:"Vision-Language-Action (VLA)"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"VLA"})," represents the cutting edge of robotics, standing for the ",(0,o.jsx)(e.strong,{children:"Vision-Language-Action"})," paradigm. It is the convergence of Large Language Models (LLMs) and physical robots, allowing a machine to understand natural language commands and translate them directly into physical movement."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h3,{id:"51-the-cognitive-pipeline-voice-to-action",children:(0,o.jsx)(e.strong,{children:"5.1 The Cognitive Pipeline: Voice-to-Action"})}),"\n",(0,o.jsx)(e.p,{children:"This pipeline describes how high-level natural language is broken down into low-level, executable robot commands."}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"The Ear (Speech Recognition):"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:["Uses models like OpenAI's ",(0,o.jsx)(e.strong,{children:"Whisper"})," to convert spoken audio into a text string."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.em,{children:"Input:"})," Audio Waveform $\\rightarrow$ ",(0,o.jsx)(e.em,{children:"Output:"}),' Text String (e.g., "Clean the room").']}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"The Brain (Cognitive Planning with LLMs):"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:["The LLM (e.g., GPT-4) acts as the ",(0,o.jsx)(e.strong,{children:"Task Planner"})," and ",(0,o.jsx)(e.strong,{children:"High-level Reasoning"})," engine."]}),"\n",(0,o.jsxs)(e.li,{children:["It takes the natural language input and breaks it down into a sequence of atomic, verifiable steps or ",(0,o.jsx)(e.strong,{children:"motion primitives"}),"."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.em,{children:"Example:"}),' The command "Clean the room" is translated into a sequence of ROS 2 actions: ',(0,o.jsx)(e.code,{children:"[navigate_to_table, locate_trash, grasp_trash, navigate_to_bin, release_trash]"}),"."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"The Body (Action Execution):"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:["The robot executes the planned sequence using the underlying ROS 2 architecture, typically relying on ",(0,o.jsx)(e.strong,{children:"Action Servers"})," for reliable, goal-oriented execution."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"52-visual-understanding-for-vla",children:(0,o.jsx)(e.strong,{children:"5.2 Visual Understanding for VLA"})}),"\n",(0,o.jsx)(e.p,{children:"To execute any step in the plan, the robot must perceive its environment."}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Object Detection:"})," Identifying specific items (e.g., trash, doors, tools) using models like YOLO or DINOv2 (often accelerated by platforms like NVIDIA Isaac ROS)."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Room Mapping:"})," Utilizing depth cameras and LiDAR to build and maintain a persistent map of the environment."]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Human-Following/Tracking:"})," Understanding the location and movement of people for coordination and safety."]}),"\n"]}),"\n",(0,o.jsx)(e.hr,{})]})}function h(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>r});var t=i(6540);const o={},s=t.createContext(o);function a(n){const e=t.useContext(s);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),t.createElement(s.Provider,{value:e},n.children)}}}]);